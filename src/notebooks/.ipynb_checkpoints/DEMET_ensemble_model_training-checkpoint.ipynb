{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPXYZFwq635F",
    "outputId": "8bac62e1-f6e2-4821-ef50-988f215f667c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TGGf7BLi7rx1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, Subset\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dHzhLdu57wrb"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = '/content/drive/MyDrive/core/models/'\n",
    "CORE_PATH = '/content/drive/MyDrive/core/'\n",
    "DATA_PATH = '/content/drive/MyDrive/core/shuffled_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk2mOgq18Exs",
    "outputId": "a7cb99b8-fd0d-4460-cc9b-16442eb25fa7"
   },
   "outputs": [],
   "source": [
    "roberta_models = [\n",
    "\n",
    "    {\n",
    "      'model' : RobertaForSequenceClassification.from_pretrained(MODEL_PATH + 'roberta/' + f'roberta-base_{i}'),\n",
    "      'tokenizer' : RobertaTokenizer.from_pretrained(MODEL_PATH + 'roberta/' + f'roberta-base_{i}')\n",
    "    }\n",
    "\n",
    "    for i in range(5)]\n",
    "\n",
    "bert_models = [\n",
    "\n",
    "    {\n",
    "      'model' : BertForSequenceClassification.from_pretrained(MODEL_PATH + 'bert/' + f'bert-base-cased_{i}'),\n",
    "      'tokenizer' : BertTokenizer.from_pretrained(MODEL_PATH + 'bert/' + f'bert-base-cased_{i}')\n",
    "    }\n",
    "\n",
    "    for i in range(5)]\n",
    "\n",
    "distil_models = [\n",
    "\n",
    "    {\n",
    "      'model' : BertForSequenceClassification.from_pretrained(MODEL_PATH + 'distilbert/' + f'distilbert-base-cased_{i}'),\n",
    "      'tokenizer' : BertTokenizer.from_pretrained(MODEL_PATH + 'distilbert/' + f'distilbert-base-cased_{i}')\n",
    "    }\n",
    "\n",
    "    for i in range(5)]\n",
    "\n",
    "ensembles = [\n",
    "    {\n",
    "        'roberta' : roberta_models[i],\n",
    "        'bert' : bert_models[i],\n",
    "        'distil' : distil_models[i]\n",
    "\n",
    "    }\n",
    "for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFH1Hpm0C2Kf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, Subset\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def plot_confusion(conf_matrix, labels, model_name):\n",
    "    title = f\"{model_name}\"\n",
    "    if not os.path.exists(CORE_PATH + str(\"plots\")):\n",
    "      os.makedirs(CORE_PATH + str(\"plots\"))\n",
    "    path = f\"{CORE_PATH}plots/{model_name}.png\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.savefig(path)\n",
    "\n",
    "def extract_logits(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        for text in dataset:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            logits.append(outputs.logits.numpy())\n",
    "    return np.vstack(logits)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "dataset = df['text'].tolist()\n",
    "labels = df['gt'].tolist()\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "trained_models = []\n",
    "individual_accuracies = []\n",
    "\n",
    "for i, ensemble in enumerate(ensembles):\n",
    "    roberta_train_logits = extract_logits(ensemble['roberta']['model'], ensemble['roberta']['tokenizer'], train_texts)\n",
    "    bert_train_logits = extract_logits(ensemble['bert']['model'], ensemble['bert']['tokenizer'], train_texts)\n",
    "    distil_train_logits = extract_logits(ensemble['distil']['model'], ensemble['distil']['tokenizer'], train_texts)\n",
    "\n",
    "    concatenated_train_logits = np.concatenate((roberta_train_logits, bert_train_logits, distil_train_logits), axis=1)\n",
    "\n",
    "    bagging_regressor = BaggingRegressor(n_estimators=10, random_state=42)\n",
    "    bagging_regressor.fit(concatenated_train_logits, train_labels)\n",
    "    trained_models.append(bagging_regressor)\n",
    "\n",
    "    roberta_test_logits = extract_logits(ensemble['roberta']['model'], ensemble['roberta']['tokenizer'], test_texts)\n",
    "    bert_test_logits = extract_logits(ensemble['bert']['model'], ensemble['bert']['tokenizer'], test_texts)\n",
    "    distil_test_logits = extract_logits(ensemble['distil']['model'], ensemble['distil']['tokenizer'], test_texts)\n",
    "\n",
    "    concatenated_test_logits = np.concatenate((roberta_test_logits, bert_test_logits, distil_test_logits), axis=1)\n",
    "\n",
    "    test_predictions = bagging_regressor.predict(concatenated_test_logits)\n",
    "    test_predictions = np.round(test_predictions)\n",
    "\n",
    "    individual_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    precision = precision_score(test_labels, test_predictions)\n",
    "    recall = recall_score(test_labels, test_predictions)\n",
    "    f1 = f1_score(test_labels, test_predictions)\n",
    "    individual_accuracies.append(individual_accuracy)\n",
    "    matrix = confusion_matrix(test_labels, test_predictions)\n",
    "    print(f\"Ensemble {i} Bagging Regressor Accuracy: {individual_accuracy}\")\n",
    "    print(f\"                               Recall: {recall}\")\n",
    "    print(f\"                               f1: {f1}\")\n",
    "    print(f\"                               Precision: {precision}\")\n",
    "    plot_confusion(matrix, ['Non-Dementia', 'Dementia'], f\"Ensemble_{i}\")\n",
    "all_predictions = np.array([model.predict(concatenated_test_logits) for model in trained_models])\n",
    "rounded_predictions = np.round(all_predictions).astype(int)\n",
    "majority_vote_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=rounded_predictions)\n",
    "\n",
    "majority_vote_accuracy = accuracy_score(test_labels, majority_vote_predictions)\n",
    "individual_accuracy = accuracy_score(test_labels, majority_vote_predictions)\n",
    "precision = precision_score(test_labels, majority_vote_predictions)\n",
    "recall = recall_score(test_labels, majority_vote_predictions)\n",
    "f1 = f1_score(test_labels, majority_vote_predictions)\n",
    "individual_accuracies.append(majority_vote_accuracy)\n",
    "matrix = confusion_matrix(test_labels, majority_vote_predictions)\n",
    "print(f\"Majority Voting Accuracy: {majority_vote_accuracy}\")\n",
    "print(f\"                               Recall: {recall}\")\n",
    "print(f\"                               f1: {f1}\")\n",
    "print(f\"                               Precision: {precision}\")\n",
    "plot_confusion(matrix, ['Non-Dementia', 'Dementia'], f\"Majority_Voting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "VuaJe-oiO3jQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, Subset\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def plot_confusion(conf_matrix, labels, model_name):\n",
    "    title = f\"{model_name}\"\n",
    "    if not os.path.exists(CORE_PATH + str(\"plots\")):\n",
    "        os.makedirs(CORE_PATH + str(\"plots\"))\n",
    "    path = f\"{CORE_PATH}plots/{model_name}.png\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.savefig(path)\n",
    "\n",
    "def extract_logits(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        for text in dataset:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            logits.append(outputs.logits.numpy())\n",
    "    return np.vstack(logits)\n",
    "\n",
    "def train_and_evaluate_model(classifier, train_features, train_labels, test_features, test_labels, model_name):\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    test_predictions = classifier.predict(test_features)\n",
    "    test_predictions = np.round(test_predictions)\n",
    "\n",
    "    individual_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    precision = precision_score(test_labels, test_predictions)\n",
    "    recall = recall_score(test_labels, test_predictions)\n",
    "    f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "    print(f\"{model_name} Accuracy: {individual_accuracy}\")\n",
    "    print(f\"             Recall: {recall}\")\n",
    "    print(f\"             F1: {f1}\")\n",
    "    print(f\"             Precision: {precision}\")\n",
    "\n",
    "    matrix = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion(matrix, ['Non-Dementia', 'Dementia'], model_name)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "dataset = df['text'].tolist()\n",
    "labels = df['gt'].tolist()\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "trained_models = []\n",
    "individual_accuracies = []\n",
    "\n",
    "classifiers = {\n",
    "    'BaggingRegressor': (BaggingRegressor(random_state=42),\n",
    "                         {'n_estimators': [10, 50, 100, 200],\n",
    "                          'max_samples': [0.5, 0.7, 1.0],\n",
    "                          'max_features': [0.5, 0.7, 1.0]}),\n",
    "\n",
    "    'RandomForestClassifier': (RandomForestClassifier(random_state=42),\n",
    "                               {'n_estimators': [50, 100, 200, 500],\n",
    "                                'max_depth': [None, 10, 20, 30, 50],\n",
    "                                'min_samples_split': [2, 5, 10],\n",
    "                                'min_samples_leaf': [1, 2, 4]}),\n",
    "\n",
    "    'GradientBoostingClassifier': (GradientBoostingClassifier(random_state=42),\n",
    "                                   {'n_estimators': [50, 100, 200, 500],\n",
    "                                    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                                    'max_depth': [3, 5, 7, 10],\n",
    "                                    'subsample': [0.7, 0.8, 0.9, 1.0]}),\n",
    "\n",
    "    'SVC': (SVC(random_state=42, probability=True),\n",
    "            {'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "             'C': [0.1, 1, 10, 100],\n",
    "             'gamma': ['scale', 'auto']}),\n",
    "\n",
    "    'KNeighborsClassifier': (KNeighborsClassifier(),\n",
    "                             {'n_neighbors': [3, 5, 7, 9],\n",
    "                              'weights': ['uniform', 'distance'],\n",
    "                              'metric': ['euclidean', 'manhattan', 'minkowski']}),\n",
    "\n",
    "    'LogisticRegression': (LogisticRegression(max_iter=1000, random_state=42),\n",
    "                           {'C': [0.01, 0.1, 1, 10, 100],\n",
    "                            'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                            'solver': ['lbfgs', 'saga']}),\n",
    "\n",
    "    'DecisionTreeClassifier': (DecisionTreeClassifier(random_state=42),\n",
    "                               {'max_depth': [None, 10, 20, 30, 50],\n",
    "                                'min_samples_split': [2, 5, 10],\n",
    "                                'min_samples_leaf': [1, 2, 4],\n",
    "                                'criterion': ['gini', 'entropy']})\n",
    "}\n",
    "\n",
    "\n",
    "roberta_train_logits = extract_logits(ensembles[0]['roberta']['model'], ensembles[0]['roberta']['tokenizer'], train_texts)\n",
    "bert_train_logits = extract_logits(ensembles[0]['bert']['model'], ensembles[0]['bert']['tokenizer'], train_texts)\n",
    "distil_train_logits = extract_logits(ensembles[0]['distil']['model'], ensembles[0]['distil']['tokenizer'], train_texts)\n",
    "\n",
    "concatenated_train_logits = np.concatenate((roberta_train_logits, bert_train_logits, distil_train_logits), axis=1)\n",
    "\n",
    "roberta_test_logits = extract_logits(ensemble['roberta']['model'], ensemble['roberta']['tokenizer'], test_texts)\n",
    "bert_test_logits = extract_logits(ensemble['bert']['model'], ensemble['bert']['tokenizer'], test_texts)\n",
    "distil_test_logits = extract_logits(ensemble['distil']['model'], ensemble['distil']['tokenizer'], test_texts)\n",
    "\n",
    "concatenated_test_logits = np.concatenate((roberta_test_logits, bert_test_logits, distil_test_logits), axis=1)\n",
    "\n",
    "for i, (classifier_name, (classifier, param_grid)) in enumerate(classifiers.items()):\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(concatenated_train_logits, train_labels)\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "\n",
    "    model_name = f\"Ensemble_{i}_{classifier_name}\"\n",
    "    train_and_evaluate_model(best_classifier, concatenated_train_logits, train_labels, concatenated_test_logits, test_labels, model_name)\n",
    "    print(f\"Best parameters for {classifier_name}: {grid_search.best_params_}\")\n",
    "    trained_models.append(best_classifier)\n",
    "\n",
    "all_predictions = np.array([classifier.predict(concatenated_test_logits) for classifier in trained_models])\n",
    "rounded_predictions = np.round(all_predictions).astype(int)\n",
    "\n",
    "majority_vote_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax() if len(x) > 0 else 0, axis=0, arr=rounded_predictions)\n",
    "\n",
    "majority_vote_accuracy = accuracy_score(test_labels, majority_vote_predictions)\n",
    "precision = precision_score(test_labels, majority_vote_predictions)\n",
    "recall = recall_score(test_labels, majority_vote_predictions)\n",
    "f1 = f1_score(test_labels, majority_vote_predictions)\n",
    "\n",
    "print(f\"Majority Voting Accuracy: {majority_vote_accuracy}\")\n",
    "print(f\"             Recall: {recall}\")\n",
    "print(f\"             F1: {f1}\")\n",
    "print(f\"             Precision: {precision}\")\n",
    "\n",
    "matrix = confusion_matrix(test_labels, majority_vote_predictions)\n",
    "plot_confusion(matrix, ['Non-Dementia', 'Dementia'], \"Majority_Voting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md4pCN0EmCbJ"
   },
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dQ2ggNUBmFq8"
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wFoGsuqxmHmG"
   },
   "outputs": [],
   "source": [
    "for i,classifier in enumerate(trained_models):\n",
    "  path = MODEL_PATH + str(i) + \".joblib\"\n",
    "  joblib.dump(classifier, path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
